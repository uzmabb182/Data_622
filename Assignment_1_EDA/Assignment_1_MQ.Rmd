---
title: "Assignment1_Data622"
author: "Mubashira Qari"
date: "2025-09-15"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
  pdf_document:
    toc: true
---

# Bank Marketing Dataset Variables  

The dataset’s main purpose is to predict whether a bank client will subscribe to a term deposit (`y = yes/no`) after a marketing campaign. Each variable provides information that may help explain or predict that outcome.  

---

## Client Information (Who the Customer Is)  

- **age (numeric)**  
  - *Recording:* Client’s age in years.  
  - *Purpose:* Age often influences financial decisions (e.g., younger clients may avoid long-term deposits, older clients may be more interested in safe investments).  

- **job (categorical)**  
  - *Recording:* Type of job (e.g., admin, blue-collar, technician).  
  - *Purpose:* Occupation reflects income stability and financial behavior.  

- **marital (categorical)**  
  - *Recording:* Marital status (single, married, divorced/widowed).  
  - *Purpose:* Family responsibilities affect savings and investment behavior.  

- **education (categorical)**  
  - *Recording:* Highest education level completed.  
  - *Purpose:* Education level may indicate financial literacy and likelihood to invest.  

- **default (categorical)**  
  - *Recording:* Whether client has credit in default ("yes", "no", "unknown").  
  - *Purpose:* Default history reflects financial risk and trustworthiness.  

- **housing (categorical)**  
  - *Recording:* Whether client has a housing loan.  
  - *Purpose:* Mortgage holders may have less disposable income for deposits.  

- **loan (categorical)**  
  - *Recording:* Whether client has a personal loan.  
  - *Purpose:* Personal loans indicate financial obligations that may reduce likelihood of subscribing.  

---

## Last Contact Information (How They Were Reached)  

- **contact (categorical)**  
  - *Recording:* Communication type ("cellular" or "telephone").  
  - *Purpose:* Some contact methods are more effective than others.  

- **month (categorical)**  
  - *Recording:* Month of the last contact.  
  - *Purpose:* Seasonal effects — campaign success may vary across the year.  

- **day_of_week (categorical)**  
  - *Recording:* Day of the week client was contacted.  
  - *Purpose:* Some days may be better for reaching clients (e.g., midweek vs. Monday).  

- **duration (numeric)**  
  - *Recording:* Call duration in seconds.  
  - *Purpose:* Longer conversations often mean higher engagement.  
  - *Note:* Duration is only known after the call, so it cannot be used in real prediction models.  

---

## Campaign History (Past Interactions)  

- **campaign (numeric)**  
  - *Recording:* Number of contacts during this campaign (including the last one).  
  - *Purpose:* Too many contacts may annoy clients, reducing success.  

- **pdays (numeric)**  
  - *Recording:* Days since last contact from a previous campaign (999 = never contacted).  
  - *Purpose:* Recency matters; recently contacted clients may behave differently.  

- **previous (numeric)**  
  - *Recording:* Number of contacts before this campaign.  
  - *Purpose:* Reflects persistence of bank marketing; may affect likelihood of success.  

- **poutcome (categorical)**  
  - *Recording:* Outcome of the previous campaign ("success", "failure", "nonexistent").  
  - *Purpose:* Past behavior often predicts future responses.  

---

## Economic Context (Overall Environment)  

- **emp.var.rate (numeric)**  
  - *Recording:* Employment variation rate (quarterly).  
  - *Purpose:* Measures labor market changes — people may invest more when jobs are stable.  

- **cons.price.idx (numeric)**  
  - *Recording:* Consumer Price Index (monthly).  
  - *Purpose:* Inflation affects purchasing power and saving behavior.  

- **cons.conf.idx (numeric)**  
  - *Recording:* Consumer Confidence Index (monthly).  
  - *Purpose:* High confidence = more willingness to invest, low confidence = caution.  

- **euribor3m (numeric)**  
  - *Recording:* Euribor 3-month interest rate.  
  - *Purpose:* Competes with deposit rates — higher Euribor may reduce deposit attractiveness.  

- **nr.employed (numeric)**  
  - *Recording:* Number of employees (quarterly labor market indicator).  
  - *Purpose:* Reflects macroeconomic health; higher employment often correlates with more savings.  

---

## Target Variable  

- **y (binary: "yes" / "no")**  
  - *Recording:* Whether the client subscribed to a term deposit.  
  - *Purpose:* This is the outcome to predict.  

---

## PART I: Exploratory Data Analysis (EDA) {.tabset}



```{r warning=FALSE}
# Load Libraries

library(dplyr)
library(tidyverse)
library(psych)
library(ggplot2)
library(plotly)
library(tidyr)
library(corrplot)
library(ggpubr)
library(naniar)     # for missing value visualization
library(DataExplorer) # optional: automated EDA
library(forcats)
library(caret)
library(recipes)
library(themis)
library(smotefamily)


```



```{r}
# Load Dataset

url <- "https://raw.githubusercontent.com/uzmabb182/Data_622/refs/heads/main/Assignment_1_EDA/bank-additional-full.csv"
bank_additional_df <- read.csv2(url, stringsAsFactors = FALSE)
head(bank_additional_df)

```

```{r}
# Basic structure
str(bank_additional_df)

```
```{r}
# Dimensions
dim(bank_additional_df)   # rows, columns
nrow(bank_additional_df)  # number of rows
ncol(bank_additional_df)  # number of columns

```
```{r}
# Column names
names(bank_additional_df)

```


```{r}
# Summary statistics for all variables
summary(bank_additional_df)

```
Are there any missing values and how significant are they? 


```{r}
# First and last few records
head(bank_additional_df, 10)
tail(bank_additional_df, 10)

# Missing values per column
missing_summary <- bank_additional_df %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percent = round(Missing_Count / nrow(bank_additional_df) * 100, 2)) %>%
  arrange(desc(Missing_Count))

missing_summary

```

## Are There Missing Values?  

The dataset does not contain raw `NA` values, but instead uses **special codes or labels** to represent “missing” or “not applicable.” These are **structural missing values** and must be considered carefully during analysis.  

---

### Where They Occur  

- **Categorical variables with `"unknown"`**  
  - *default:* `"unknown"` is very common (many clients do not disclose credit default history).  
  - *education:* includes an `"unknown"` category (~5% of records).  
  - *job:* contains a small number of `"unknown"` entries.  

- **Special numeric code – `pdays = 999`**  
  - Indicates the client was **not previously contacted**.  
  - Dominates the column (~96% of rows).  
  - Not truly “missing,” but a **placeholder code** that should be treated as its own category.  

---

### Significance of Missingness  

- **default = "unknown":** Very significant because it covers a large fraction of the data. Dropping it would result in too much information loss, so it should be treated as its own level (“missing info”).  
- **education = "unknown":** Smaller but still relevant; may need grouping with other low-frequency categories.  
- **job = "unknown":** Rare and not very impactful, but still worth encoding properly.  
- **pdays = 999:** Extremely significant since it applies to almost all clients. If not handled correctly, it can distort model training.  

---

### Final Answer  

Yes, there are missing values, but they appear as **coded placeholders** rather than raw `NA`:  

- `"unknown"` in categorical variables (*default, education, job*).  
- `pdays = 999` meaning “not previously contacted.”  

These placeholders are highly significant because they cover a large portion of the dataset (especially *pdays* and *default*). Instead of dropping them, they should be **treated as meaningful categories or carefully recoded** for modeling.  


```{r}
# Unique values in categorical variables (factor/character columns)
lapply(bank_additional_df[sapply(bank_additional_df, is.character)], unique)

```


```{r}
library(dplyr)
library(tidyr)

# Select only character (categorical) columns
categorical_df <- bank_additional_df %>% select(where(is.character))

# Using describe () for summary statsw
describe(categorical_df)


```

What is the overall distribution of each variable?

```{r}
library(ggplot2)
library(dplyr)
library(forcats)
library(viridis)

# Ensure correct types
bank_additional_df <- bank_additional_df %>%
  mutate(
    euribor3m = as.numeric(euribor3m),  # force to numeric
    nr.employed = as.numeric(nr.employed),
    emp.var.rate = as.numeric(emp.var.rate),
    cons.price.idx = as.numeric(cons.price.idx),
    cons.conf.idx = as.numeric(cons.conf.idx)
  )

# Separate categorical and numeric columns
categorical_df <- bank_additional_df %>% select(where(is.character))
numeric_df     <- bank_additional_df %>% select(where(is.numeric))

# --- Plot categorical variables (one by one) ---
for (col in names(categorical_df)) {
  p <- categorical_df %>%
    ggplot(aes(x = fct_infreq(.data[[col]]))) +
    geom_bar(fill = viridis(1, begin = 0.3, end = 0.8), alpha = 0.8) +
    coord_flip() +   # flip for readability
    theme_minimal() +
    theme(
      axis.text.y = element_text(size = 9),
      axis.title.y = element_blank(),
      axis.title.x = element_text(size = 11),
      plot.title = element_text(size = 14, face = "bold")
    ) +
    ylab("Frequency") +
    ggtitle(paste("Distribution of", col))
  
  print(p)
}

# --- Plot numeric variables (histogram + density) ---
for (col in names(numeric_df)) {
  p <- numeric_df %>%
    ggplot(aes(x = .data[[col]])) +
    geom_histogram(aes(y = ..density..), bins = 40,
                   fill = viridis(1, begin = 0.3, end = 0.8), color = "white") +
    geom_density(color = "red", size = 0.8) +
    theme_minimal() +
    ggtitle(paste("Distribution of", col)) +
    ylab("Density") +
    xlab(col)
  
  print(p)
}


```


## How Are Categorical Variables Distributed?  

### Job  
- Bars show how many clients belong to each job type.  
- **Most common:** *admin.*, *blue-collar*, and *technician*.  
- **Rare categories:** *illiterate* and *unknown* (very small bars).  

---

### Marital Status  
- Categories: *married*, *single*, *divorced*.  
- **Married** is the dominant group.  

---

### Education  
- Compares counts of *basic*, *high.school*, and *university.degree*.  
- Reveals which education level is most common in the dataset.  

---

### Default / Housing / Loan  
- Variables have levels: *yes / no / unknown*.  
- **Default:** Vast majority are *no* or *unknown*; very few *yes*.  
- **Housing loan:** Many *yes*, but also a large portion *no*.  
- **Personal loan:** Mostly *no*; *yes* is rare.  
- In all cases, *no* overwhelmingly dominates.  

---

### Contact Method  
- *cellular* vs *telephone*.  
- In later campaigns, **cellular dominates**, so that bar is higher.  

---

### Month / Day of Week  
- Show seasonal patterns of campaign activity.  
- **Peak months:** May, August, July, November → tall bars.  
- Day of week distribution shows midweek contacts were more common.  

---

### Previous Outcome (poutcome)  
- Outcome of the previous marketing campaign.  
- Almost all are *nonexistent* → one very tall bar.  
- *Failure* and *success* are very short.  

---

### Target Variable (y)  
- *yes* vs *no* (subscription to term deposit).  
- Strong class imbalance: **~88% no vs. ~12% yes**.  

---

##  Interpretation Takeaways  

- **Imbalance:** Most categorical features are highly imbalanced (e.g., *default*, *poutcome*, *y*).  
- **Dominant categories:** Certain levels dominate distributions (e.g., *married* in marital, *cellular* in contact).  
- **Modeling insight:** These plots highlight the need for **rebalancing, proper encoding, or collapsing rare categories** before building predictive models.  


```{r}

# Correlation matrix for numeric columns
numeric_vars <- bank_additional_df[sapply(bank_additional_df, is.numeric)]
cor(numeric_vars, use = "complete.obs")

```
## What Are the Relationships Between Different Variables?  
Are the features (columns) of the dataset correlated?  

---

### Pairwise Correlations  

- **age & duration (-0.00087):** Essentially zero; the client’s age has no linear relationship with call duration.  
- **age & campaign (0.0046):** Nearly zero; older clients are not contacted more or less often.  
- **age & pdays (-0.034):** Very weak negative correlation; older clients are slightly more likely to have been contacted recently in previous campaigns, but the effect is negligible.  
- **age & previous (0.024):** Essentially no correlation; age does not relate to prior contacts.  

- **duration & campaign (-0.072):** Very weak negative correlation; longer calls are slightly associated with fewer contacts in this campaign.  
- **duration & pdays (-0.048):** Very weak negative correlation; call duration is not meaningfully related to days since last contact.  
- **duration & previous (0.021):** Almost zero; prior contacts do not affect call length.  

- **campaign & pdays (0.053):** Very weak positive correlation; clients contacted longer ago may have slightly more contacts in this campaign.  
- **campaign & previous (-0.079):** Very weak negative correlation; more prior contacts are slightly associated with fewer contacts in this campaign.  

- **pdays & previous (-0.588):** Moderate to strong negative correlation; as days since last contact (pdays) increases, the number of prior contacts decreases. This makes sense: if someone was contacted long ago, there were fewer previous contacts.  

---

### Key Takeaways  

- **Most variables show very weak correlations** (close to 0), meaning they are largely independent.  
- **The only strong relationship** is between *pdays* and *previous* (-0.588), which is meaningful for modeling.  
- **Variables such as age, duration, and campaign** are not strongly correlated with each other, so **multicollinearity is unlikely** among them.  

```{r}
library(tidyverse)

# bank <- bank_additional_df  # your data

bank <- bank_additional_df %>%
  mutate(
    y = factor(y, levels = c("no","yes")),
    y_num = as.numeric(y) - 1
  )

numeric_candidates <- c(
  "age","duration","campaign","pdays","previous",
  "emp.var.rate","cons.price.idx","cons.conf.idx","euribor3m","nr.employed","y_num"
)

bank_num <- bank %>%
  mutate(across(all_of(intersect(names(bank), numeric_candidates)),
                ~ suppressWarnings(as.numeric(.)))) %>%
  select(any_of(numeric_candidates)) %>%
  select(where(~ !all(is.na(.))))

cmat <- cor(bank_num, use = "complete.obs")

# Long format for ggplot
corr_long <- as.data.frame(as.table(cmat)) %>%
  setNames(c("Var1","Var2","value"))

ggplot(corr_long, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "#2166AC", mid = "white", high = "#B2182B",
                       midpoint = 0, limits = c(-1, 1), name = "corr") +
  geom_text(aes(label = sprintf("%.2f", value)), size = 3) +
  labs(title = "Correlation Heatmap (numeric features + y_num)", x = NULL, y = NULL) +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
## Correlation Heatmap Interpretation  

The heatmap displays **Pearson correlation coefficients** between numeric features (ranging from –1 to +1).  

- **Red:** strong positive correlation  
- **Blue:** strong negative correlation  
- **White:** near zero (no linear relationship)  
- **Numbers inside each tile:** exact correlation values  

---

### Key Observations  

- **Target Variable (y_num)**  
  - Weak correlations overall (|r| < 0.4).  
  - Slight positive relationship with *duration* (0.41) and *nr.employed* (0.35).  
  - Weak negative relationships with *pdays* (–0.32) and *previous* (–0.23).  
  - *Interpretation:* Subscription outcome is only weakly associated with individual numeric features.  

- **Strong Positive Correlations**  
  - *euribor3m* and *nr.employed* (0.97).  
  - *euribor3m* and *emp.var.rate* (0.95).  
  - *emp.var.rate* and *nr.employed* (0.91).  
  - *cons.price.idx* and *euribor3m* (0.88).  
  - *Interpretation:* These economic indicators move together, showing redundancy. Dimensionality reduction or feature selection may be needed.  

- **Strong Negative Correlations**  
  - *pdays* and *previous* (–0.59).  
  - *emp.var.rate* and *cons.conf.idx* (–0.53).  
  - *euribor3m* and *cons.conf.idx* (–0.53).  
  - *Interpretation:* Clients contacted long ago had fewer prior contacts; consumer confidence moves opposite to employment variation and interest rates.  

- **Near-Zero Correlations**  
  - *Age* with nearly all variables (–0.01 to 0.13).  
  - *Campaign* with most others (–0.08 to 0.15).  
  - *Interpretation:* These features are largely independent.  

---

### Takeaways  

- **y_num** has only weak direct correlations → predictive strength will likely come from interactions or non-linear effects.  
- **Economic variables (euribor3m, nr.employed, emp.var.rate, cons.price.idx)** are highly correlated → risk of multicollinearity; may need PCA or dropping redundant features.  
- **pdays and previous** show a meaningful negative relationship (–0.59) → consistent with campaign structure.  
- **Most other features are independent**, reducing concerns of multicollinearity outside of the economic block.  

Overall, the heatmap highlights **clusters of correlated economic indicators** and a few notable structural relationships, but confirms that **most client-level features are weakly correlated**. This suggests feature engineering (e.g., creating flags or buckets) may be more useful than relying on raw numeric values.  


```{r}
# Install if needed
# install.packages("naniar")
# install.packages("ggplot2")

library(dplyr)
library(ggplot2)
library(naniar)

# Copy dataset
bank_missing <- bank_additional_df

# Recode special placeholders as NA
bank_missing <- bank_missing %>%
  mutate(across(where(is.character), ~ na_if(., "unknown"))) %>% 
  mutate(pdays = ifelse(pdays == 999, NA, pdays))

# Single barplot: percentage of missing values per variable
gg_miss_var(bank_missing, show_pct = TRUE) +
  ggtitle("Percentage of Missing Values per Variable") +
  ylab("Percentage Missing") +
  theme_minimal()


```



```{r}

# Outlier Detection
# Load libraries
# Outlier Detection
# Load libraries
library(dplyr)
library(tidyr)
library(ggplot2)
library(viridis)

# --- 1) Fix numeric-like character columns safely ---
bank_fixed <- bank_additional_df %>%
  mutate(
    across(c(emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed),
           ~ as.numeric(.x))   # convert if not already numeric
  )

# --- 2) Select numeric columns ---
numeric_df <- bank_fixed %>% select(where(is.numeric))

# --- 3) Reshape to long format ---
numeric_long <- numeric_df %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

# --- 4) Faceted boxplots ---
ggplot(numeric_long, aes(x = variable, y = value, fill = variable)) +
  geom_boxplot(outlier.color = "red", outlier.shape = 16, alpha = 0.6) +
  facet_wrap(~ variable, scales = "free", ncol = 3) +   # separate panels
  scale_fill_viridis(discrete = TRUE, guide = "none") +
  theme_minimal(base_size = 13) +
  labs(title = "Outlier Detection Across Numeric Variables",
       x = "", y = "Value") +
  theme(
    strip.text = element_text(face = "bold", size = 11),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )


```
# Quick Refresher: Boxplot Guide  

- **Center line:** median  
- **Box:** IQR (Q1–Q3)  
- **Whiskers:** last data points within Q1 − 1.5·IQR and Q3 + 1.5·IQR  
- **Dots:** outliers (beyond those fences)  

*Note:* This figure uses free scales per panel, so don’t compare absolute heights across panels.  

---

## Interpretation of Boxplots  

### Age  
- Most ages fall between **45–60 years**.  
- Median age: **~50**.  
- Outliers: a few very high ages (80–100+). Not errors, but much higher than majority.  

### Campaign  
- Represents number of contacts made to a client during a campaign.  
- Middle 50% of values: **1–3 contacts**.  
- Median: ~**1–2 contacts**.  
- Outliers: Many clients contacted **10–40+ times** → unusual.  

### Consumer Confidence Index (cons.conf.idx)  
- Values: –45 to –37, median ~ –41 to –42.  
- Indicates generally pessimistic sentiment.  
- Outlier: one less negative value (~ –35).  

### Consumer Price Index (cons.price.idx)  
- Values tightly clustered: **93.5–94.0**, median ~93.8.  
- Very stable — no outliers.  

### Duration (Seconds)  
- Represents last call duration.  
- Middle 50%: a few hundred seconds (minutes).  
- Median: **under 500–600 sec (~10 minutes)**.  
- Outliers: Many calls lasted **2000–5000 seconds (1+ hour)**.  

### Employment Variation Rate (emp.var.rate)  
- Range: –2% to +1%.  
- Median: ~0%.  
- Whiskers: –3.5% to +1.5%.  
- Compact distribution, no extreme outliers.  

### Euribor 3m  
- Median: ~3%.  
- Middle 50%: **2%–4%**.  
- Range: 1%–5%.  
- No outliers.  

### Number Employed (nr.employed)  
- Median: ~5160.  
- Middle 50%: **5100–5200**.  
- Stable, tight distribution.  

### Pdays  
- Special code = **999** (never contacted before).  
- Dominates distribution (~96% of clients).  
- Outliers: very small number of clients with values near 0.  
- Should be treated as categorical: *contacted before vs not contacted*.  

### Previous  
- Median: 0.  
- Most clients never contacted before.  
- Outliers: a few clients contacted **1–7 times**.  

---

## Relationship Between Variables  

Even if variables are not correlated, they can be combined into meaningful features.  

- **Age × Job:** Older clients in certain jobs may respond differently. → `age * job`.  
- **Loan status + Housing loan:** Clients with both loans may signal financial stress. → `any_loan`.  
- **Duration × Outcome (y):** Long calls often mean higher engagement.  
- **Economic indicators:** Combine `emp.var.rate`, `cons.price.idx`, `cons.conf.idx`, `euribor3m`, `nr.employed` into an index.  
- **Pdays + Previous:** Recently contacted & frequent contacts may behave differently.  

**Answer in short:**  
Yes, even if variables are not correlated, they can be combined into new, meaningful features. Example: height + weight → BMI. In this dataset: housing loan + personal loan → overall debt indicator. Domain knowledge guides feature engineering.  

---

## Do Any Patterns or Trends Emerge?  

- **Marketing months:** Contacts peak in May, Aug, Jul, Nov.  
- **Contact duration:** Longer calls → higher likelihood of subscription.  
- **Loans:** Clients without loans/defaults slightly more likely to subscribe.  
- **Age & job:** Middle-aged (30–50) and professionals dominate.  
- **Economic indicators:** Lower euribor3m and better emp.var.rate → higher subscriptions.  
- **Target imbalance:** Term deposit subscription is rare (~12%).  

**Summary:** Campaigns are seasonal, call length matters, and both demographics and macroeconomics influence outcomes.  

---

## Central Tendency and Spread  

- **Age:** Mean ~40, median 38, range 17–98.  
- **Duration:** Mean ~258 sec, median ~180 sec, skewed with long outliers.  
- **Campaign:** Median 2, mean ~2.6, max 56.  
- **Pdays:** Median 999, mean ~962.  
- **Previous:** Median 0, mean 0.17, max 7.  
- **emp.var.rate:** –3 to +1.  
- **cons.price.idx:** 92–94.  
- **cons.conf.idx:** –50 to –25.  
- **euribor3m:** 0.6–5.0.  
- **nr.employed:** 4960–5228.  

**Summary:** Duration, campaign, pdays, previous are skewed. Economic indicators are stable.  

---

## Are There Missing Values?  

No raw `NA`s, but several variables use **special placeholders**:  

- **default = "unknown"** (~20%) → many clients do not disclose credit default history.  
- **education = "unknown"** (~5%) → a smaller but noticeable fraction of clients.  
- **job = "unknown"** (~1%) → minimal impact.  
- **pdays = 999** (~96% of clients) → not truly missing, indicates “never contacted before.”  

**Summary:**  
The dataset contains **structural missing values** rather than random NA’s. These should be **treated as valid categories or transformed into features** (e.g., `pdays = 999` → “not previously contacted”), instead of dropping rows, to avoid losing important information.  

---

# PART II: Pre-Processing {.tabset}  

### A. Data Cleaning  
- Replace `"unknown"` with explicit category *missing*.  
- Treat `pdays=999` as *not previously contacted*.  
- Keep outliers but apply transformations (e.g., log for regression).  
- Remove duplicates if present.  

### B. Dimensionality Reduction  
- Drop/reduce highly correlated economic indicators (emp.var.rate, euribor3m, nr.employed).  
- Logistic regression sensitive to multicollinearity; decision trees less so.  

### C. Feature Engineering  
- **pdays:** create `was_contacted_before` (binary) + `pdays_num` (numeric if not 999).  
- **Loans:** create `any_loan`.  
- **Age groups:** youth <30, middle 30–50, senior >50.  
- **Interactions:** duration × contact type.  

### D. Sampling  
- Large dataset (~41k), so no down-sampling needed.  
- Handle imbalance (~11% “yes”) with SMOTE, undersampling, or class weights.  

### E. Transformation  
- Normalize numeric variables for regression.  
- One-hot encode categoricals (keep `"unknown"`).  
- Log transform skewed features (`duration`, `campaign`, `previous`).  

### F. Imbalanced Data  
- Use SMOTE or class weights.  
- Evaluate with Precision, Recall, F1 (not just accuracy).  

**Final Preprocessing Summary**  
1. Handled structural missing values.  
2. Retained but transformed meaningful outliers.  
3. Reduced redundancy in economic indicators.  
4. Engineered features (e.g., any_loan, was_contacted_before).  
5. Prepared dataset for balanced, interpretable modeling.  

---


```{r warning=FALSE}
library(dplyr)
library(forcats)
library(caret)
library(smotefamily)

# --- 1. Load Data ---
url <- "https://raw.githubusercontent.com/uzmabb182/Data_622/main/Assignment_1_EDA/bank-additional-full.csv"
bank <- read.csv2(url, stringsAsFactors = FALSE)

# --- 2. Clean Data ---

# For the columns listed, turn them into numeric columns
# For all character columns (except y), replace the word 'unknown' with 'missing' and then make them categorical factors
# Turn 999 in the pdays column into proper missing values
# Make sure no duplicate records remain
# The target variable (y) is categorical with two values: no/yes, and I want ‘no’ to come first.

num_char_cols <- c("emp.var.rate","cons.price.idx","cons.conf.idx","euribor3m","nr.employed")

df <- bank %>%
  mutate(across(all_of(num_char_cols), ~ suppressWarnings(as.numeric(.)))) %>%
  mutate(across(where(is.character) & !matches("^y$"),
                ~ factor(ifelse(. == "unknown", "missing", .)))) %>%
  mutate(pdays = ifelse(pdays == 999, NA_integer_, pdays)) %>%
  distinct()

df$y <- factor(df$y, levels = c("no","yes"))

# --- 3. Feature Engineering ---

# Checks if the column pdays is missing (NA).

# If missing → assign 0 (never contacted before).

# If not missing → assign 1 (contacted before).

# Stored as integer (0L or 1L).

df <- df %>%
  mutate(
    was_contacted_before = ifelse(is.na(pdays), 0L, 1L),
    pdays_num            = ifelse(is.na(pdays), NA_integer_, pdays),
    any_loan             = ifelse(loan == "yes" | housing == "yes", "yes", "no"),
    age_group            = case_when(
      age < 30 ~ "youth",
      age <= 50 ~ "middle",
      TRUE ~ "senior"
    ),
    duration_contact     = duration * ifelse(contact == "cellular", 1, 0),
    duration_log         = log1p(duration),
    campaign_log         = log1p(campaign),
    previous_log         = log1p(previous)
  )

# --- 4. Split target and predictors ---
y <- df$y
predictors <- df %>% select(-y)

# --- 5. One-hot encode + impute + scale (recipe-style with caret) ---
dmy <- dummyVars(" ~ .", data = predictors)
X <- data.frame(predict(dmy, newdata = predictors))

# Impute + scale
pre <- preProcess(X, method = c("medianImpute","center","scale"))
X_imp <- predict(pre, X)

# Check consistency
stopifnot(nrow(X_imp) == length(y))
cat("Rows in X_imp:", nrow(X_imp), " | Rows in y:", length(y), "\n")

# --- 6. Handle Imbalance ---
## Option A: SMOTE
set.seed(123)
sm <- SMOTE(X_imp, y, K = 5)
df_smote <- sm$data
df_smote$y <- factor(df_smote$class, levels = c("no","yes"))
df_smote$class <- NULL
table(df_smote$y)

## Option B: Upsampling
set.seed(123)
up <- upSample(x = X_imp, y = y, yname = "y")
table(up$y)

```
## Data Preprocessing Steps

The following steps were applied to prepare the dataset for modeling:

1. **Identify numeric columns**  
   - Converted columns that should be numeric into proper numeric types.

2. **Handle categorical values**  
   - Replaced `"unknown"` text with `"missing"`.  
   - Converted categorical columns into factors.

3. **Special case: `pdays`**  
   - Changed `pdays = 999` into proper missing values (`NA`).

4. **Remove duplicates**  
   - Dropped duplicate rows to ensure data quality.

5. **Target variable**  
   - Made the target column `y` a categorical factor with levels `"no"` and `"yes"`.

6. **Encoding and transformation**  
   - Used `caret::dummyVars()` for one-hot encoding. This approach is safer and preserves all rows.  
   - Applied `preProcess()` *after* dummy encoding to handle imputation (e.g., `pdays_num`) before scaling and centering.

7. **Final consistency check**  
   - Ensured that the number of rows in predictors and target matched:  
     - `nrow(X_imp)` = `length(y)` = **41,176 rows**.

---


```{r}
# visual checks to confirm class balance before and after SMOTE / Upsampling.

library(ggplot2)

# --- Original balance ---
orig_tbl <- as.data.frame(table(y))
colnames(orig_tbl) <- c("Class","Count")

ggplot(orig_tbl, aes(x = Class, y = Count, fill = Class)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = Count), vjust = -0.5) +
  labs(title = "Class Balance - Original Data", y = "Count") +
  theme_minimal()

# --- After SMOTE ---
smote_tbl <- as.data.frame(table(df_smote$y))
colnames(smote_tbl) <- c("Class","Count")

ggplot(smote_tbl, aes(x = Class, y = Count, fill = Class)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = Count), vjust = -0.5) +
  labs(title = "Class Balance - After SMOTE", y = "Count") +
  theme_minimal()

# --- After Upsampling ---
up_tbl <- as.data.frame(table(up$y))
colnames(up_tbl) <- c("Class","Count")

ggplot(up_tbl, aes(x = Class, y = Count, fill = Class)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = Count), vjust = -0.5) +
  labs(title = "Class Balance - After Upsampling", y = "Count") +
  theme_minimal()

```


```{r}
# Split into Train/Test

set.seed(123)
trainIndex <- createDataPartition(y, p = 0.7, list = FALSE)

# Original
X_train <- X_imp[trainIndex, ]
X_test  <- X_imp[-trainIndex, ]
y_train <- y[trainIndex]
y_test  <- y[-trainIndex]


```


```{r}
# Train Logistic Regression (Original)

model_orig <- glm(y_train ~ ., data = data.frame(y_train, X_train), family = binomial)
pred_orig  <- predict(model_orig, newdata = X_test, type = "response")
pred_class_orig <- ifelse(pred_orig > 0.5, "yes", "no")

confusionMatrix(factor(pred_class_orig, levels=c("no","yes")), y_test)


```


```{r}
# Train Logistic Regression (SMOTE)

# Split SMOTE data
set.seed(123)
trainIndex_smote <- createDataPartition(df_smote$y, p = 0.7, list = FALSE)

train_smote <- df_smote[trainIndex_smote, ]
test_smote  <- df_smote[-trainIndex_smote, ]

model_smote <- glm(y ~ ., data = train_smote, family = binomial)
pred_smote  <- predict(model_smote, newdata = test_smote, type = "response")
pred_class_smote <- ifelse(pred_smote > 0.5, "yes", "no")

confusionMatrix(factor(pred_class_smote, levels=c("no","yes")), test_smote$y)


```


```{r}
# Train Logistic Regression (Upsampled)

# Split Upsampled data
set.seed(123)
trainIndex_up <- createDataPartition(up$y, p = 0.7, list = FALSE)

train_up <- up[trainIndex_up, ]
test_up  <- up[-trainIndex_up, ]

model_up <- glm(y ~ ., data = train_up, family = binomial)
pred_up  <- predict(model_up, newdata = test_up, type = "response")
pred_class_up <- ifelse(pred_up > 0.5, "yes", "no")

confusionMatrix(factor(pred_class_up, levels=c("no","yes")), test_up$y)

```

## Model Performance Comparison

- **Original dataset**  
  - Produces high overall accuracy.  
  - However, recall for `"yes"` is poor because the model predicts `"no"` most of the time due to class imbalance.

- **SMOTE (Synthetic Minority Oversampling Technique)**  
  - Recall for `"yes"` improves since synthetic examples balance the dataset.  
  - Provides more diverse synthetic cases compared to simple duplication.

- **Upsampling**  
  - Recall also improves by balancing the dataset.  
  - However, it can lead to overfitting since it duplicates minority class examples instead of creating new ones.



# PART III: Algorithm Selection {.tabset}  

### Business Context  
- **Goal:** predict subscription (y = yes/no).  
- **Size:** ~41k records, mixed variables.  
- **Challenge:** Class imbalance (~11% yes).  
- **Need:** Balance interpretability and predictive power.  

### Candidate Algorithms  

**1. Logistic Regression**  
- Pros: interpretable coefficients, probability outputs, efficient.  
- Cons: assumes linear log-odds, requires preprocessing, may miss nonlinearities.  

**2. Decision Tree**  
- Pros: handles numeric/categorical, captures interactions, intuitive rules.  
- Cons: prone to overfitting, less stable, lower accuracy than ensembles.  

**3. Naïve Bayes (secondary)**  
- Pros: fast, works with categorical data, probabilistic.  
- Cons: assumes independence, less accurate on structured business data.  

---

### Recommended Algorithm  
- **Primary:** Logistic Regression → interpretability + regulatory alignment.  
- **Secondary:** Decision Tree → captures nonlinear patterns, generates actionable rules.  

---

### Responses to Questions  

1. **Are there labels?**  
   Yes → y (yes/no). It’s supervised classification.  

2. **How does algorithm choice relate to data?**  
   Large, imbalanced dataset with mixed variables → logistic regression + decision tree fit well.  

3. **What if dataset < 1,000 records?**  
   Prefer simpler models (logistic regression, LDA, Naïve Bayes). Decision trees may be unstable.  

---

### Justification  
1. **Data characteristics:** supervised, binary classification with imbalance → logistic regression fits well.  
2. **Problem type:** supervised learning, classification.  
3. **Business needs:** interpretability and actionable insights → logistic regression + decision tree.  

---

### Final Answer  
I recommend **Logistic Regression** as the primary model for interpretability and alignment with business context, with **Decision Trees** as a complementary method to capture nonlinear patterns.  
