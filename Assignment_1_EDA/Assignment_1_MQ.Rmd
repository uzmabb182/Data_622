---
title: "Assignment_1_Data_622"
author: "Mubashira Qari"
date: "2025-09-20"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

Bank Marketing Dataset Variables

The dataset’s main purpose is to predict whether a bank client will subscribe to a term deposit (y = yes/no) after a marketing campaign. Each variable is information that may help explain or predict that outcome.

Client Information (who the customer is)

1.	age (numeric)
What it is recording: Client’s age in years.
Why included: Age often influences financial decisions (e.g., younger clients may avoid long-term deposits, older clients may be more interested in safe investments).
2.	job (categorical)
	Recording: Type of job (e.g., admin, blue-collar, technician).
	Purpose: Occupation reflects income stability and financial behavior.
3.	marital (categorical)
	Recording: Marital status (single, married, divorced/widowed).
	Purpose: Family responsibilities affect savings and investment behavior.
4.	education (categorical)
	Recording: Highest education level completed.
	Purpose: Education level may indicate financial literacy and likelihood to invest.
5.	default (categorical)
	Recording: Whether client has credit in default ("yes", "no", "unknown").  – yes means he/she has not returned money in past and not trustable
	Purpose: Default history reflects financial risk — important for targeting reliable clients.
6.	housing (categorical)
	Recording: Whether client has a housing loan.
	Purpose: Mortgage holders may have less disposable income for deposits.
7.	loan (categorical)
	Recording: Whether client has a personal loan.
	Purpose: Personal loans indicate financial obligations that may reduce likelihood of subscribing.

Last Contact Information (how they were reached)
8.	contact (categorical)
	Recording: Communication type ("cellular" or "telephone").
	Purpose: Some contact methods are more effective than others.
9.	month (categorical)
	Recording: Month of the last contact.
	Purpose: Seasonal effects — campaign success may vary across the year.
10.	day_of_week (categorical)
	Recording: Day of the week client was contacted.
	Purpose: Some days may be better for reaching clients (e.g., midweek vs. Monday).
11.	duration (numeric)
	Recording: Call duration in seconds.
	Purpose: Longer conversations often mean higher engagement.  But: this is only known after the call, so it cannot be used in a real prediction model.
Campaign History (past interactions)
12.	campaign (numeric)
	Recording: Number of contacts during this campaign (including last one).
	Purpose: Too many contacts may annoy clients, reducing success.
13.	pdays (numeric)
	Recording: Days since last contact from a previous campaign (999 = never contacted).
	Purpose: Recency matters; recently contacted clients may behave differently.
14.	previous (numeric)
	Recording: Number of contacts before this campaign.
	Purpose: Reflects persistence of bank marketing; may affect likelihood of success.
15.	poutcome (categorical)
	Recording: Outcome of the previous campaign ("success", "failure", "nonexistent").
	Purpose: Past behavior often predicts future responses.
Economic Context (overall environment)
16.	emp.var.rate (numeric)
	Recording: Employment variation rate (quarterly).
	Purpose: Measures labor market changes — people may invest more when jobs are stable.
17.	cons.price.idx (numeric)
	Recording: Consumer Price Index (inflation indicator, monthly).
	Purpose: Inflation affects purchasing power and saving behavior.
18.	cons.conf.idx (numeric)
	Recording: Consumer Confidence Index (monthly).
	Purpose: High confidence = more willingness to invest, low confidence = caution.
19.	euribor3m (numeric)
	Recording: Euribor 3-month interest rate.
	Purpose: Competes with deposit rates — higher Euribor may reduce deposit attractiveness.
20.	nr.employed (numeric)
	Recording: Number of employees (quarterly labor market indicator).
	Purpose: Reflects macroeconomic health; higher employment often correlates with more savings.
Target Variable
21.	y (binary: "yes","no")
	Recording: Whether the client subscribed to a term deposit.
	Purpose: This is the outcome to predict.
Summary:
This dataset combines personal info, contact history, and economic indicators to help banks understand which clients are most likely to subscribe to a term deposit. It records both individual-level behavior and macro-level conditions.


# Exploratory Data Analysis:

Review the structure and content of the data and answer questions such as:
Are the features (columns) of your data correlated?
What is the overall distribution of each variable?
Are there any outliers present?
What are the relationships between different variables?
How are categorical variables distributed?
Do any patterns or trends emerge in the data?
What is the central tendency and spread of each variable?
Are there any missing values and how significant are they? 


```{r warning=FALSE}
# Load Libraries

library(dplyr)
library(tidyverse)
library(psych)
library(ggplot2)
library(plotly)
library(tidyr)
library(corrplot)
library(ggpubr)
library(naniar)     # for missing value visualization
library(DataExplorer) # optional: automated EDA
```



```{r}
# Load Dataset

url <- "https://raw.githubusercontent.com/uzmabb182/Data_622/refs/heads/main/Assignment_1_EDA/bank-additional-full.csv"
bank_additional_df <- read.csv2(url, stringsAsFactors = FALSE)
head(bank_additional_df)

```

```{r}
# Basic structure
str(bank_additional_df)

```
```{r}
# Dimensions
dim(bank_additional_df)   # rows, columns
nrow(bank_additional_df)  # number of rows
ncol(bank_additional_df)  # number of columns

```
```{r}
# Column names
names(bank_additional_df)

```


```{r}
# Summary statistics for all variables
summary(bank_additional_df)

```
Are there any missing values and how significant are they? 


```{r}
# First and last few records
head(bank_additional_df, 10)
tail(bank_additional_df, 10)

# Missing values per column
missing_summary <- bank_additional_df %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percent = round(Missing_Count / nrow(bank_additional_df) * 100, 2)) %>%
  arrange(desc(Missing_Count))

missing_summary

```

Are there missing values?

The dataset does not contain raw NA values, but instead uses special codes / labels to represent “missing” or “not applicable.”

These act as structural missing values and are important for analysis.

Where they occur:

Categorical variables with "unknown":

default → “unknown” is very common (many clients don’t disclose credit default history).

education → has an “unknown” category (~5% of records).

job → small number of “unknown” entries.

Special numeric code (pdays = 999):

This means the client was not previously contacted.

It dominates the column (≈ 96% of rows).

Not truly “missing,” but a placeholder code that should be treated as a separate category.

Significance of Missingness:

default = "unknown" → significant, because it represents a large fraction of data (if treated as NA and dropped, you’d lose too much information). Best to treat as its own level (“missing info”).

education = "unknown" → smaller but still relevant; may need grouping with other low-frequency categories.

job = "unknown" → rare, not too impactful.

pdays = 999 → extremely significant; almost all clients fall here. If not handled correctly, it can distort models.

Final Answer:

Yes, there are missing values, but they appear as coded placeholders rather than raw NA:

"unknown" in categorical variables (default, education, job).

pdays = 999 meaning “not previously contacted.”

These are highly significant because they cover a large portion of the dataset (especially pdays and default). Instead of dropping them, they should be treated as meaningful categories or carefully recoded for modeling.

```{r}
# Unique values in categorical variables (factor/character columns)
lapply(bank_additional_df[sapply(bank_additional_df, is.character)], unique)

```


```{r}
library(dplyr)
library(tidyr)

# Select only character (categorical) columns
categorical_df <- bank_additional_df %>% select(where(is.character))

# Using describe () for summary statsw
describe(categorical_df)


```

What is the overall distribution of each variable?

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(forcats)
library(viridis)

# Overall distribution of each variable

# Select categorical columns
categorical_df <- bank_additional_df %>% select(where(is.character))

# Loop through each categorical variable and plot separately
for (col in names(categorical_df)) {
  
  p <- categorical_df %>%
    ggplot(aes(x = fct_infreq(.data[[col]]))) +
    geom_bar(fill = viridis(1, begin = 0.3, end = 0.8), alpha = 0.8) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
      axis.title.x = element_blank(),
      axis.title.y = element_text(size = 11),
      plot.title = element_text(size = 14, face = "bold")
    ) +
    ylab("Frequency") +
    ggtitle(paste("Distribution of", col))
  
  print(p)  # Shows each plot one after another
}



```

How are categorical variables distributed?

Distribution of job:

Bars show how many clients belong to each job type.

Expect “admin.,” “blue-collar,” and “technician” to be tallest bars (most common).

Rare categories like “illiterate” or “unknown” will have very small bars.

Distribution of marital:

Shows how many are “married,” “single,” “divorced.”

Likely “married” dominates.

Distribution of Education:

Compares counts of “basic,” “high.school,” “university.degree.”

Reveals which education level is most common in the dataset.

Distribution of default / housing / loan:

Yes/no/unknown variables.

Default → Vast majority “no” or “unknown”; very few “yes”.

Housing loan → Many “yes”, but a large portion “no”.

Loan (personal loan) → Mostly “no”; “yes” is rare.

Typically, “no” is overwhelmingly higher (few clients default, fewer have loans).

Distribution of contact Method:

“cellular” vs “telephone.”

In later campaigns, “cellular” dominates, so that bar is higher.

Distribution of month / day_of_week:

Shows seasonal patterns of when campaigns were run.

For example, May, Aug, Jul, Nov are peak campaign months → tall bars.

Distribution of poutcome:

Outcome of previous marketing campaigns.

Almost all “nonexistent” → one very tall bar, “failure” and “success” very short.

Distribution of y (target variable):

“yes” vs “no” (subscription to term deposit).

Strong class imbalance: “no” is much taller than “yes” (~88% vs 12%).

Interpretation Takeaways:

Imbalance: Most categorical features are highly imbalanced (e.g., default, poutcome, y).

Dominant categories: Certain levels dominate (e.g., “married” in marital, “cellular” in contact).

Insights for modeling: These plots tell where we need rebalancing, encoding, or collapsing categories before feeding into a model.

```{r}
# overall distribution of each variable

# Load packages
library(dplyr)
library(tidyr)
library(ggplot2)
library(forcats)
library(readr)
library(viridis)

# --- 1) Fix numeric-like character columns ---
bank_fixed <- bank_additional_df %>%
  mutate(
    across(c(emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed),
           ~ suppressWarnings(parse_number(.x)))
  )

# --- 2) Numeric variables ---
numeric_df <- bank_fixed %>% select(where(is.numeric))

for (col in names(numeric_df)) {
  p <- ggplot(bank_fixed, aes(x = .data[[col]])) +
    geom_histogram(bins = 30, fill = viridis(1, begin = 0.3, end = 0.8), color = "white", alpha = 0.8) +
    theme_minimal(base_size = 13) +
    labs(title = paste("Distribution of", col),
         x = col, y = "Frequency")
  print(p)
}

# --- 3) Categorical variables ---
categorical_df <- bank_fixed %>% select(where(~is.character(.x) || is.factor(.x)))

for (col in names(categorical_df)) {
  p <- ggplot(bank_fixed, aes(x = fct_infreq(as.factor(.data[[col]])))) +
    geom_bar(fill = viridis(1, begin = 0.3, end = 0.8), alpha = 0.8) +
    theme_minimal(base_size = 13) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = paste("Distribution of", col),
         x = col, y = "Count")
  print(p)
}

```


```{r}

# Correlation matrix for numeric columns
numeric_vars <- bank_additional_df[sapply(bank_additional_df, is.numeric)]
cor(numeric_vars, use = "complete.obs")

```


age & duration (-0.00087):	Essentially zero correlation; age of client has no linear relationship with call duration.
age & campaign (0.0046):	Nearly zero; older clients are not contacted more or less often.
age & pdays (-0.034):	Very weak negative correlation; older clients slightly more likely to have been contacted recently in previous campaigns, but effect is negligible.
age & previous (0.024):	Essentially no correlation; age does not relate to prior contacts.
duration & campaign (-0.072):	Very weak negative correlation; longer calls slightly associated with fewer calls in this campaign.
duration & pdays (-0.048):	Very weak negative correlation; call duration not meaningfully related to days since last contact.
duration & previous (0.021):	Almost zero; prior contacts do not affect call length.
campaign & pdays (0.053):	Very weak positive correlation; number of contacts in this campaign is slightly higher for clients contacted longer ago.
campaign & previous (-0.079):	Very weak negative correlation; more prior contacts slightly associated with fewer contacts in this campaign.
pdays & previous (-0.588):	Moderate to strong negative correlation; as days since last contact (pdays) increases, the number of prior contacts decreases. Makes sense: if someone was contacted long ago, there were fewer previous contacts.

Key takeaways:

Most variables have very weak correlations (close to 0), meaning they are largely independent.

The only strong relationship is between pdays and previous (-0.588), which is meaningful for modeling.

Variables like age, duration, and campaign are not strongly correlated with each other, so multicollinearity is unlikely among them.

======================================================================================================================

What are the relationships between different variables? Are the features (columns) of your data correlated?


```{r}
library(tidyverse)

# bank <- bank_additional_df  # your data

bank <- bank_additional_df %>%
  mutate(
    y = factor(y, levels = c("no","yes")),
    y_num = as.numeric(y) - 1
  )

numeric_candidates <- c(
  "age","duration","campaign","pdays","previous",
  "emp.var.rate","cons.price.idx","cons.conf.idx","euribor3m","nr.employed","y_num"
)

bank_num <- bank %>%
  mutate(across(all_of(intersect(names(bank), numeric_candidates)),
                ~ suppressWarnings(as.numeric(.)))) %>%
  select(any_of(numeric_candidates)) %>%
  select(where(~ !all(is.na(.))))

cmat <- cor(bank_num, use = "complete.obs")

# Long format for ggplot
corr_long <- as.data.frame(as.table(cmat)) %>%
  setNames(c("Var1","Var2","value"))

ggplot(corr_long, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "#2166AC", mid = "white", high = "#B2182B",
                       midpoint = 0, limits = c(-1, 1), name = "corr") +
  geom_text(aes(label = sprintf("%.2f", value)), size = 3) +
  labs(title = "Correlation Heatmap (numeric features + y_num)", x = NULL, y = NULL) +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
Each tile shows the Pearson correlation coefficient between two numeric variables (from –1 to +1).

Colors:

Blue = negative correlation (closer to –1)

Red = positive correlation (closer to +1)

White = near 0 (no linear relationship)

Numbers inside tiles are the exact correlation values.

Key Observations:

Age vs Others:

Very close to 0 with all variables (≈0.01–0.04).

Interpretation: Age is largely independent; older/younger clients don’t systematically differ in campaign, pdays, etc.

Duration vs Campaign / Pdays / Previous

All correlations are very weak (≈ –0.03 to 0.04).

Slight negative with Previous (≈ –0.06).

Interpretation: Call length is not strongly tied to campaign persistence or past contact history.

Campaign vs Previous:

Essentially 0.

Interpretation: Number of contacts in the current campaign is independent of how many times the client was contacted before.

Pdays vs Previous:

Very close to 0, though structurally most are (pdays=999, previous=0).

Interpretation: Linear correlation is low, but in reality this is a categorical relationship (flag-like).

Overall:

All correlations are weak (< |0.1|).

Interpretation: None of these numeric variables have linear dependence. Predictive value will come from non-linear patterns, bins, or flags, not from raw correlations.

Takeaway

The heatmap confirms: no strong linear relationships exist among the numeric features.

Structural features matter more:

pdays == 999 → “never contacted before” flag.

duration > threshold → long vs short calls.

campaign > 3 → many attempts.

For modeling, feature engineering (e.g., indicators, buckets, interactions) will be more powerful than relying on raw linear correlations.

=========================================================================================================================================

Are there any outliers present?

```{r}
# Outlier Detection

# Load libraries
library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
library(viridis)

# --- 1) Fix numeric-like character columns ---
bank_fixed <- bank_additional_df %>%
  mutate(
    across(c(emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed),
           ~ suppressWarnings(parse_number(.x)))
  )

# --- 2) Select numeric columns ---
numeric_df <- bank_fixed %>% select(where(is.numeric))

# --- 3) Reshape to long format ---
numeric_long <- numeric_df %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

# --- 4) Faceted boxplots ---
ggplot(numeric_long, aes(x = variable, y = value, fill = variable)) +
  geom_boxplot(outlier.color = "red", outlier.shape = 16, alpha = 0.6) +
  facet_wrap(~ variable, scales = "free", ncol = 3) +   # separate panels
  scale_fill_viridis(discrete = TRUE, guide = "none") +
  theme_minimal(base_size = 13) +
  labs(title = "Outlier Detection Across Numeric Variables",
       x = "", y = "Value") +
  theme(
    strip.text = element_text(face = "bold", size = 11),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )



```
**Quick refresher first:**

Center line = median.

Box = IQR (Q1–Q3).

Whiskers = last data points within Q1 − 1.5·IQR and Q3 + 1.5·IQR.

Dots = outliers (beyond those fences).

This figure uses free scales per panel, so don’t compare absolute heights across panels.

**Interpretation of Plots**

Age: 

Most ages fall between about 45 and 60 years old (the purple box).

The median age is around 50.

There are some very high ages (above ~80–100) that are considered statistical outliers. These don’t necessarily mean errors, but they are much higher than the majority of the population.

Campaign:

It represents the number of contacts made to a client during a campaign.

The Box (black rectangle) shows the middle 50% of the data (between the 25th and 75th percentiles).

Here, the box is very close to 0, meaning that for most clients, the number of contacts was quite small (probably 1–3 contacts).

The Median (line inside the box) lies near 1 or 2, so typically clients were contacted just once or twice.

The Whiskers extend to the largest "normal" values. These are still relatively low compared to the outliers.

The Red Dots (outliers) are many. Some people were contacted 10, 20, 30, even 40+ times, which is far beyond the majority. These points are unusual, so they’re marked as outliers.

Consumer Confidence Index (cons.conf.idx):

It measures how optimistic or pessimistic consumers are about the economy.

The Box (blue rectangle) represents the middle 50% of values (between the 25th percentile and 75th percentile).

Here, most values range from -45 to about -37.

The Median (line inside the box) sits around -41 to -42, meaning the "typical" consumer confidence is quite low (pessimistic).

The Whiskers extend to the lowest and highest values that are not outliers.

Here, they cover roughly -50 to -35.

The Red Dot (outlier) a value just above -35, which is less negative than the rest.

This means there was a rare period where consumers were slightly more optimistic compared to the usual pessimistic trend.

Consumer Price Index (cons.price.idx):

It measures inflation — the average change in prices consumers pay for goods and services.

The Box (blue rectangle) represents the middle 50% of the values.

Here, values are tightly clustered between 93.5 and 94.0.

The Median (line inside the box) sits right around 93.8–93.9, which is the "typical" CPI in this dataset.

The Whiskers (vertical lines) extend slightly above and below the box, but not very far.

This shows that there isn’t much variation in CPI — prices were quite stable.

No Outliers - this one doesn’t show any red dots.

That means all CPI values fall within a reasonable range — no unusual inflation/deflation spikes.

The consumer price index in this dataset is very stable, with little variation, clustered tightly around 93.8.

Duration (Seconds):

This boxplot represents the variable duration, which usually refers to the duration of the last contact with a client (in seconds) in marketing/banking datasets.

The Box (black rectangle) captures the middle 50% of call durations.

Here, most calls lasted a few hundred seconds (a few minutes).

The Median (line inside the box) - The typical call length is relatively short (under 500–600 seconds, i.e., under ~10 minutes).

The Whiskers (vertical lines) - Show the reasonable range of call lengths.

Calls longer than this range are flagged as unusual.

The Red Dots (outliers) - There are many! Some calls lasted 2000, 3000, even 4000–5000 seconds which is more than an hour and are marked as outliers.

We need to decide how to handle these extreme durations (outliers) when preparing dataset for machine learning.


Employment Variation Rate (emp.var.rate):

It measures the change in employment levels compared to the previous quarter.

Positive values → employment is increasing (job market is stronger).

Negative values → employment is decreasing (job market is weaker).

So this variable gives context about the economic environment at the time of the bank campaign call.

Box (IQR range): Most of the values are between roughly –2% and +1%. 

This reflects that during the campaign period, some quarters had sharp employment declines, while others had slight growth (+1).

Median (center line): Around 0%, meaning half the values are below 0 and half above.

Whiskers: Extend from about –3.5% to +1.5%, showing the full spread of typical observations.

Outliers: None are marked beyond the whiskers in this plot → the distribution looks fairly compact.


euribor3m: (3-month Euro Interbank Offered Rate)

It’s the interest rate at which European banks lend money to each other for 3 months.

Median (center line): around 3%.

IQR (box): most values lie between ~2% and ~4%.

Whiskers: extend roughly from 1% to 5%.

Outliers: None visible → values stay within a normal range.


nr.employed (number of employees in the economy - employment index):

It reflects the overall labor market size and conditions during the campaign period.

Median (center line): around 5,160 employees (index value).

IQR (box): most values lie between ~5,100 and 5,200.

Whiskers: extend slightly lower, near 5,000.

Outliers: None — the distribution is very tight and stable.


pdays (Number of days since the client was last contacted in a previous marketing campaign):

This boxplot shows the variable pdays, which refers to the number of days since a client was last contacted in a previous campaign.

The Box and Whisker (long black line at the top) - The majority of data points are at 999, meaning most clients had not been previously contacted.

This dominates the distribution, so the box is pushed to the high end.

The Red Dot (outlier near 0) - This represents clients who were recently contacted (within a few days).

These cases are statistically rare compared to the overwhelming 999 values, so they appear as outliers.

Treat 999 as a separate category (not a number).

If using for modeling: We can create a binary feature: was_contacted_before = (pdays != 999).

Keep actual numeric values for clients with real pdays


previous (number of contacts performed before this campaign for a given client):

It tells us how many times the client was approached in earlier marketing campaigns.

Box (IQR): Collapsed at 0 → meaning the vast majority of clients were never contacted before.

Median: 0.

Outliers (red dots): Small number of clients had between 1 and 7 prior contacts, which show as outliers.

#==========================================================================================================================================


Relationship between variables (this is more of a domain knowledge  (2) question compared to #1 i.e. even if variables are not correlated can they be combined e.g. height & weight combined to be new feature BMI?)

Some possibilities in the dataset:

Age × Job

Older clients in certain jobs (e.g., retired, management) may respond differently to marketing than younger people in the same jobs.

Interaction term: age * job.

Loan status + Housing loan

Having both a personal loan and a housing loan may signal financial stress or risk.

Could engineer a combined feature: has_any_loan = (loan == "yes" | housing == "yes").

Contact duration + Outcome (y)

Long calls may indicate more engaged clients.

Duration itself is skewed, but combined with outcome it can be used to model conversion likelihood.

Economic indicators

emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed

Instead of using them separately, you could combine into an economic sentiment index or capture interactions (e.g., interest rates matter more when unemployment is high).

pdays + previous

A client contacted recently (low pdays) and contacted multiple times before (high previous) may behave differently from someone new.

Interaction: recently_contacted & frequent_contact.

General Guidance

Don’t rely only on correlation → correlation measures linear pairwise relationships, but useful combinations may be non-linear or conditional.

Think in terms of domain knowledge → which variables together describe meaningful behavior?

Use transformations/interactions → ratios, differences, products, categorical combinations.

Validate with models → after creating new features, check if they improve predictive power (e.g., using logistic regression, random forests, or feature importance).

Answer in short:
Yes, even if variables are not correlated, they can be combined to form new, meaningful features. For example, height and weight → BMI, or in your dataset, housing loan + personal loan → overall debt indicator. The key is to use domain knowledge to create features that capture real-world relationships, then test whether they improve prediction.

# ========================================================================================================================================

Do any patterns or trends emerge in the data?

Marketing months: Contacts are not spread evenly across the year; most campaigns were in May, Aug, Jul, Nov.

Contact duration: Longer calls tend to correspond to clients who subscribed (y = yes).

Loans: Clients without loans or defaults are slightly more likely to subscribe.

Age & job: Middle-aged clients (30–50) and professionals (admin., management) dominate the dataset.

Economic indicators: Lower euribor3m (interest rates) and better emp.var.rate often coincide with higher subscription rates.

Target imbalance: The strong skew in y shows that term deposit subscription is relatively rare.

Summary: Campaigns are seasonal, calls are short but long calls may matter, and economic conditions + demographics influence client behavior.

# ========================================================================================================================================

What is the central tendency and spread of each variable?

(Numeric variables summary from your dataset snapshot)

Age → Mean ~40, median 38, spread 17–98 years.

Duration → Mean ~258 sec, median ~180 sec, highly skewed (outliers up to ~4,918 sec).

Campaign → Median 2, mean ~2.6, right-skewed with max 56.

Pdays → Median 999 (never contacted), mean ~962, range 0–999.

Previous → Median 0, mean ~0.17, max 7.

Emp.var.rate → Small range (economic index), values around -3 to +1.

Cons.price.idx → Range ~92–94.

Cons.conf.idx → Range ~ -50 to -25.

Euribor3m → Range ~0.6–5.0.

Nr.employed → Range ~4960–5228.

Summary:

Strong skewness in duration, campaign, pdays, previous.

Central tendency (mean/median) differs significantly for skewed variables.

Economic indicators are more stable and smooth.

# =============================================================================================================================================

Are there any missing values and how significant are they?

No true NA values reported in the dataset, but some categories encode “missing” as text:

default = "unknown" (quite common, ~20%).

education = "unknown" (~5%).

job = "unknown" (~1%).

pdays = 999 is a special missing code, meaning “not previously contacted” (~96% of clients).

Summary:

No raw NA’s, but “unknown” and “999” act as structural missing values.

Most clients were never contacted before (pdays = 999).

Handling “unknown” categories is important (drop, treat as separate class, or impute).

# ===========================================================================================================================================

Algorithm Selection for Bank Marketing Dataset
Business Context
The business goal is to predict whether a client subscribes to a term deposit.
•	Target variable (y): yes / no → a binary label.
•	Dataset size: ~41,000 records with both numeric and categorical features.
•	Challenge: Class imbalance (only ~11–12% “yes”).
•	Requirement: A model that balances predictive accuracy with interpretability so bank managers can act on the insights.


Candidate Algorithms
1. Logistic Regression
•	Pros
Interpretable coefficients (odds ratios) → managers can see how features (e.g., age, housing loan, call duration) impact probability of subscription.
Outputs probabilities → supports customer ranking/prioritization.
Efficient on large datasets; easy to implement.
•	Cons
Assumes linear relationship in log-odds space.
Requires preprocessing (dummy coding for categorical variables).
May underperform if nonlinear interactions are important.
 Consistency with EDA: Our EDA showed that features like call duration, campaign count, and prior contact history strongly affect outcomes. Logistic regression can directly quantify these relationships and provide interpretable results.


2. Decision Tree
•	Pros
Naturally handles both numeric and categorical variables.
Captures nonlinear patterns and feature interactions automatically.
Produces intuitive “if–then” rules → easy for marketing managers to interpret.
•	Cons
Single trees prone to overfitting.
Less stable: small changes in data can change splits.
Accuracy generally lower than ensemble methods.
 Consistency with EDA: The EDA revealed nonlinear relationships and skewed distributions (e.g., many clients never previously contacted, pdays=999). A decision tree can handle these patterns directly and translate them into actionable rules (e.g., “Clients with long call duration and no loan → higher likelihood of subscription”).


3. Naïve Bayes (secondary option)
•	Pros
Very fast and scalable to large datasets.
Works well with categorical features.
Provides simple probabilistic interpretation.
•	Cons
Assumes conditional independence among features (rarely holds in practice).
Less accurate than logistic regression or decision trees on structured business data.


 Recommended Algorithm
•	Primary choice: Logistic Regression → Best balance of interpretability, regulatory friendliness, and handling of class imbalance with probability thresholds.
•	Secondary choice: Decision Tree → Complements logistic regression by capturing nonlinear patterns and generating clear business rules.


Responses to Specific Questions
1.	Are there labels in your data? Did that impact your choice of algorithm?
Yes → y (yes/no) is a labeled binary target.
This makes it a supervised classification problem, guiding the choice toward logistic regression and decision trees.
2.	How does your choice of algorithm relate to the dataset?
Dataset is large (40k+) and imbalanced. Logistic regression is efficient at scale, while decision trees naturally handle mixed data types and outliers. Both align well with the dataset’s structure.
3.	Would your choice of algorithm change if there were fewer than 1,000 records, and why?
With fewer records, simpler models like Logistic Regression, LDA, or Naïve Bayes would be safer to avoid overfitting.
Decision trees can become unstable on very small datasets. Logistic regression would remain the preferred option.


 Justification Based on Rubric
1.	Data characteristics, constraints & insights (10):
Binary classification, mix of categorical/numeric features, large dataset with imbalance. Logistic regression fits binary data well; decision trees capture nonlinearities observed in EDA.
2.	Problem Type (Supervised vs. Unsupervised) (5):
This is supervised learning with labels → classification models are appropriate (not PCA or other unsupervised methods).
3.	Business insight (5):
Banks need interpretability to justify marketing actions to regulators and managers. Logistic regression provides transparency; decision trees provide intuitive rule sets.


 Final Answer (1 line)
I recommend Logistic Regression as the primary model for its interpretability and business alignment, with Decision Trees as a complementary model to capture nonlinear patterns, given this is a supervised classification problem with labeled binary data and imbalanced classes.


